{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d480c196",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) is a branch of machine learning that focuses on how agents can learn to make decisions through trial and error to maximize cumulative rewards. \n",
    "\n",
    "RL allows machines to learn by interacting with an environment and receiving feedback based on their actions. \n",
    "\n",
    "This feedback comes in the form of rewards or penalties.\n",
    "\n",
    "![  ](https://media.geeksforgeeks.org/wp-content/uploads/20250903150649221420/Reinforecement-Learning-in-ML.webp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5e6e4",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Reinforcement Learning is based on the concept of a learner, called an agent, exploring and acting within a setting known as the environment to accomplish a specific objective.\n",
    "\n",
    "As the agent takes steps or makes choices, it receives signals from the environment that indicate how well it’s doing.\n",
    "\n",
    "Over repeated interactions, the agent uses this feedback to refine its strategy and make better decisions.\n",
    "\n",
    "**Agent:** The intelligent entity that makes decisions or takes actions.  \n",
    "\n",
    "**Environment:** The external world or system the agent interacts with.  \n",
    "\n",
    "**State:** The current situation or context the agent finds itself in at any moment.  \n",
    "\n",
    "**Action:** A move or decision the agent can carry out in a given state.  \n",
    "\n",
    "**Reward:** A numerical signal the environment sends back to the agent after each action, reflecting the immediate consequence of that action guiding the agent toward desirable behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01384e83",
   "metadata": {},
   "source": [
    " the core components of Reinforcement Learning\n",
    "\n",
    "1. Policy\n",
    "\n",
    "    Defines the agent’s behavior i.e maps states for actions.\n",
    "    Can be simple rules or complex computations.\n",
    "    Example: An autonomous car maps pedestrian detection to make necessary stops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e8c29",
   "metadata": {},
   "source": [
    "2. Reward Signal\n",
    "\n",
    "    Represents the goal of the RL problem.\n",
    "    Guides the agent by providing feedback (positive/negative rewards).\n",
    "    Example: For self-driving cars rewards can be fewer collisions, shorter travel time, lane discipline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec4bbd",
   "metadata": {},
   "source": [
    "3. Value Function\n",
    "\n",
    "    Evaluates long-term benefits, not just immediate rewards.\n",
    "    Measures desirability of a state considering future outcomes.\n",
    "    Example: A vehicle may avoid reckless maneuvers (short-term gain) to maximize overall safety and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797f05c",
   "metadata": {},
   "source": [
    "4. Model\n",
    "\n",
    "    Simulates the environment to predict outcomes of actions.\n",
    "    Enables planning and foresight.\n",
    "    Example: Predicting other vehicles’ movements to plan safer routes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eccf279",
   "metadata": {},
   "source": [
    "Working of Reinforcement Learning\n",
    "\n",
    "The agent interacts iteratively with its environment in a feedback loop:\n",
    "\n",
    "    The agent observes the current state of the environment.\n",
    "\n",
    "    It chooses and performs an action based on its policy.\n",
    "    \n",
    "    The environment responds by transitioning to a new state and providing a reward (or penalty).\n",
    "    \n",
    "    The agent updates its knowledge (policy, value function) based on the reward received and the new state.\n",
    "    \n",
    "    This cycle repeats with the agent balancing exploration (trying new actions) and exploitation (using known good actions) to maximize the cumulative reward over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c703512",
   "metadata": {},
   "source": [
    "This process is mathematically framed as a **Markov Decision Process (MDP)** where future states depend only on the current state and action, not on the prior sequence of events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbeca31",
   "metadata": {},
   "source": [
    "Markov Decision Process\n",
    "\n",
    "**Markov Decision Process (MDP)** is a way to describe how a decision-making agent like a robot or game character moves through different situations while trying to achieve a goal. MDPs rely on variables such as the environment, agent’s actions and rewards to decide the system’s next optimal action. It helps us answer questions like:\n",
    "\n",
    "    What actions should the agent take?\n",
    "    What happens after an action?\n",
    "    Is the result good or bad?\n",
    "\n",
    "In artificial intelligence Markov Decision Processes (MDPs) are used to model situations where decisions are made one after another and the results of actions are uncertain. \n",
    "\n",
    "They help in designing smart machines or agents that need to work in environments where each action might led to different outcomes.\n",
    "\n",
    "An MDP has five main parts:\n",
    "\n",
    "1. **States (S)** – All possible situations the environment can be in. The agent observes the current state to decide what to do next.\n",
    "\n",
    "2. **Actions (A)** – The set of choices or moves available to the agent in each state.\n",
    "\n",
    "3. **Transition Probabilities (P)** – The likelihood of ending up in a new state after taking an action in a given state. Formally:  \n",
    "   \\( P(s' \\mid s, a) = \\text{probability of moving to state } s' \\text{ from state } s \\text{ after action } a \\).\n",
    "\n",
    "4. **Reward Function (R)** – The immediate feedback (a number) the agent receives after performing an action in a state. It tells the agent how good or bad the outcome was. Often written as \\( R(s, a) \\) or \\( R(s, a, s') \\).\n",
    "\n",
    "5. **Discount Factor (γ)** – A value between 0 and 1 that determines how much the agent values future rewards compared to immediate ones. A higher γ means the agent plans for the long term.\n",
    "\n",
    "Together, these form the MDP tuple: **⟨S, A, P, R, γ⟩**.  \n",
    "This framework allows an agent to learn an optimal **policy** a rule for selecting actions that maximizes total expected reward over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f810a94",
   "metadata": {},
   "source": [
    "Applications\n",
    "\n",
    "Markov Decision Processes are useful in many real-life situations where decisions must be made step-by-step under uncertainty. Here are some applications:\n",
    "\n",
    "**Robots and Machines:** Robots use MDPs to decide how to move safely and efficiently in places like factories or warehouses and avoid obstacles.\n",
    "\n",
    "**Game Strategy:** In board games or video games MDPs help characters to choose the best moves to win or complete tasks even when outcomes are not certain.\n",
    "\n",
    "**Healthcare:** Doctors can use it to plan treatments for patients, choosing actions that improve health while considering uncertain effects.\n",
    "\n",
    "**Traffic and Navigation:** Self-driving cars or delivery vehicles use it to find safe routes and avoid accidents on unpredictable roads.\n",
    "\n",
    "**Inventory Management:** Stores and warehouses use MDPs to decide when to order more stock so they don’t run out or keep too much even when demand changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f1a1b4",
   "metadata": {},
   "source": [
    "**Implementing Reinforcement Learning**\n",
    "\n",
    "Let's see the working of reinforcement learning with a maze example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b05f2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14d54aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FrozenLake environment (4x4 grid)\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)  # Deterministic for simplicity\n",
    "\n",
    "# Initialize Q-table: Q[state, action] = value\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1      # Learning rate\n",
    "gamma = 0.99     # Discount factor\n",
    "epsilon = 0.9    # Exploration rate\n",
    "episodes = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9ba55db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "\n",
      "Learned Q-table:\n",
      "[[0.94 0.95 0.93 0.94]\n",
      " [0.94 0.   0.51 0.85]\n",
      " [0.75 0.26 0.03 0.28]\n",
      " [0.19 0.   0.01 0.  ]\n",
      " [0.95 0.96 0.   0.94]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.   0.91 0.   0.09]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.96 0.   0.97 0.95]\n",
      " [0.96 0.98 0.98 0.  ]\n",
      " [0.88 0.99 0.   0.75]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.   0.98 0.99 0.97]\n",
      " [0.98 0.99 1.   0.98]\n",
      " [0.   0.   0.   0.  ]]\n",
      "\n",
      "Testing policy:\n",
      "\n",
      "Total reward: 1\n"
     ]
    }
   ],
   "source": [
    "# Q-Learning algorithm\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q[state])        # Exploit\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Q-learning update\n",
    "        Q[state, action] = Q[state, action] + alpha * (\n",
    "            reward + gamma * np.max(Q[next_state]) - Q[state, action]\n",
    "        )\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "    # Decay epsilon (less exploration over time)\n",
    "    epsilon = max(0.01, epsilon * 0.999)\n",
    "\n",
    "# Test the learned policy\n",
    "print(\"Training completed!\")\n",
    "print(\"\\nLearned Q-table:\")\n",
    "print(Q.round(2))\n",
    "\n",
    "# Test the learned policy\n",
    "print(\"\\nTesting policy:\")\n",
    "state, _ = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(Q[state])\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    env.render()\n",
    "\n",
    "print(f\"\\nTotal reward: {total_reward}\")\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b5a34e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total reward: 1\n",
      "Path taken: [0, 4, 8, 9, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "# Test with SLOW animation\n",
    "env_test = gym.make('FrozenLake-v1', is_slippery=False, render_mode='human')\n",
    "state, _ = env_test.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "path = [state]\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(Q[state])\n",
    "    state, reward, terminated, truncated, _ = env_test.step(action)\n",
    "    path.append(state)\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    time.sleep(0.9)  # ⏱️ Pause for 0.5 seconds between steps\n",
    "\n",
    "env_test.close()\n",
    "print(f\"\\nTotal reward: {total_reward}\")\n",
    "print(f\"Path taken: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab41a4",
   "metadata": {},
   "source": [
    "What This Does:\n",
    "1. **Environment**: `FrozenLake-v1`  \n",
    "   - 4x4 grid with **S** (start), **F** (frozen), **H** (hole), **G** (goal)  \n",
    "   - Agent must reach **G** without falling into **H**  \n",
    "   - Actions: `0=Left`, `1=Down`, `2=Right`, `3=Up`\n",
    "\n",
    "2. **Algorithm**: **Q-Learning**  \n",
    "   - Learns a `Q-table` mapping `(state, action) → expected future reward`\n",
    "   - Uses **epsilon-greedy** exploration\n",
    "   - Updates Q-values using:  \n",
    "     \\( Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\right] \\)\n",
    "\n",
    "3. **Output**:  \n",
    "   - Trained Q-table  \n",
    "   - Visual path taken during test (using `env.render()`)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
